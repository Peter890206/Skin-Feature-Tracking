# Cotracker

## Summary

This folder contains the codes of Co-tracker.
We only use **Peter_demo.py** to track the point in our project.

[add by Peter]

## Environment Setup



## How to use?

1. Firstly, please check you are in the correct environment.

2. Check you have the suitable data which can be generated by following the instructions in the README.md in "nordlinglab-digitalupdrs\Process_Video_Deep_Learning_Tracking_Peter\Preprocess\preprocess_for_cotracker".

3. Change the input video path and the output name.

4. Change the initial tracking coordinates in the **Peter_demo.py**.

5. Using `python -m torch.distributed.run --nproc_per_node=1 Peter_demo.py` to run the code.

6. It will generate two csv file (each one record one point, you can track two points at the same time) including the tracking coordinates for each frame.

## Notes:

- The code now only can save max two point coordinates, if you want to save more, please modify the **visualizer.py** in "./cotracker/utils/".

- The input video size is limited to 400*400 pixels, and the length of the video is limited to 10 seconds (2400 frames).

- The distributed GPUS is not supported yet, so you can only use one GPU but you still need to use `python -m torch.distributed.run --nproc_per_node=1` to run.



