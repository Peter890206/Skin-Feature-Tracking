# Cotracker

## Summary

This folder contains the codes of Co-tracker.
We only use **Peter_demo.py** to track the point in our project.

[add by Peter]

## Environment Setup

Using docker to build the image.
Download and load the image tar file.
The image.tar file can be found in the folder "DigitalUPDRS/QPD_Shared/Peter_DFE_Docker_Env" in our NAS.
You can just download it and use following command to load the image:
    `sudo docker load -i cotracker_image.tar`
Once you build the image, you can run the following command to run the image:
    `sudo docker run -it --gpus=all --ipc=host -v (your folder path):(folder path in the container) --name=(your container name) cotracker_image bash`
If you want to check the command detailly, please check the presentation in "NordlingLab/All_Presentations/Presentations_Lab/Peter_docker_introduction_231003.pptx" in our NAS.

## How to use?

1. Firstly, please check you are in the correct environment.

2. Check you have the suitable data which can be generated by following the instructions in the README.md in "nordlinglab-digitalupdrs\Process_Video_Deep_Learning_Tracking_Peter\Preprocess\preprocess_for_cotracker".

3. Change the input video path and the output name.

4. Change the initial tracking coordinates in the **Peter_demo.py**.

5. Using `python -m torch.distributed.run --nproc_per_node=1 Peter_demo.py` to run the code.

6. It will generate two csv file (each one record one point, you can track two points at the same time) including the tracking coordinates for each frame.

## Notes:

- The code now only can save max two point coordinates, if you want to save more, please modify the **visualizer.py** in "./cotracker/utils/".

- The input video size is limited to 400*400 pixels, and the length of the video is limited to 10 seconds (2400 frames).

- The distributed GPUS is not supported yet, so you can only use one GPU but you still need to use `python -m torch.distributed.run --nproc_per_node=1` to run.



